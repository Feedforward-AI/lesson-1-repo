# PROPOSAL — CFO Review Comments

**Original Document:** Meridian AI Lab: Building the Future Today
**Submitted by:** Jordan Torres, Head of AI Initiative
**Reviewed by:** Gene Hollister, CFO
**Date of Review:** October 15, 2025

---

*Note: Comments added via document review prior to October Steering Committee. Jordan — sharing these so you can see my concerns before the meeting. I want to support this if I can, but I need more than what's here. — Gene*

---

## Executive Summary

> I'm proposing we create a dedicated AI Lab: a small team of high performers with access to frontier AI tools and a mandate to experiment, learn, and spread knowledge across Meridian.

**[GH]:** "Experiment, learn, and spread knowledge" — these are activities, not outcomes. What does the business get? How does this show up on the P&L?

> This isn't about replacing our current approach. It's about creating a parallel track where we can move faster, learn more, and develop the organizational muscle we'll need as AI capabilities accelerate.

**[GH]:** Why do we need a parallel track? Why not fix the current approach? This reads like we're admitting failure on the $8M we already spent, but not learning from it.

> **Investment ask:** $400K incremental annual cost

**[GH]:** It's not $400K. It's $400K + 50-60% of 8 high performers' time. What's the loaded cost of those FTEs? If we're backfilling their roles at $180K, we're only covering a fraction. The real investment is north of $800K when you account for opportunity cost.

> **Timeline:** 12-month initial commitment with quarterly reviews

**[GH]:** What happens at each quarterly review? What would cause us to stop? I don't see kill criteria anywhere in this document.

---

## The Problem We're Solving

> We're stuck.
>
> Our current approach—enterprise-wide deployment of a "safe" tool (EnterpriseAI.ai) with extensive governance—has produced:
> - 37% adoption
> - Minimal workflow transformation
> - Growing frustration from our most capable people
> - Zero competitive advantage

**[GH]:** Fair enough on the 37% adoption figure. But I agree the impact is minimal. However, the diagnosis matters. Is the problem the tool? The governance? The use cases? The training? This proposal assumes it's the tool, but I'm not convinced.

> Meanwhile, the technology is advancing faster than our evaluation process. By the time we approve a tool, it's already a generation behind.

**[GH]:** Maybe. But "a generation behind" doesn't mean useless. We're not a tech company. We don't need the newest model to summarize meeting notes.

> I've talked to dozens of employees across Meridian. The consistent theme: "I want to use AI, but the tools we're given aren't good enough, and the tools I want to use aren't allowed."

**[GH]:** Which employees? What functions? "Dozens" is anecdotal. I'd want to see systematic survey data before accepting this as representative.

> We've created a system optimized for risk avoidance that's actually generating a different kind of risk: the risk of falling behind.

**[GH]:** This is an assertion, not an argument. What evidence is there that we're "falling behind"? Behind whom? By what measure? What's the cost of that gap? I keep hearing this but no one quantifies it.

---

## The Proposal

### What

> A dedicated team of 10 people, working with frontier AI tools, tasked with:
>
> 1. **Discovering** high-value AI applications across Meridian's functions
> 2. **Developing** prototypes and proof-of-concept implementations
> 3. **Documenting** what works, what doesn't, and why
> 4. **Disseminating** knowledge across the organization
> 5. **De-risking** future enterprise deployments by learning in a contained environment

**[GH]:** Again — activities, not outcomes. I don't fund activities. I fund outcomes.

Here's what I need: "After 12 months, we will have delivered [X specific result] that produces [Y measurable business impact]." Give me three concrete examples of what success looks like, with numbers.

### Who

> **8 "Star Performers"** selected from across the organization

**[GH]:** Who selects them? What's the appeals process? What if a manager refuses to release someone? This is going to create political problems. Pam should weigh in on the talent management implications.

> The star performers aren't leaving their roles entirely—this is 50-60% of their time, with their regular work backfilled or redistributed.

**[GH]:** "Backfilled or redistributed" — which is it? If backfilled, the $180K in the budget doesn't cover 8 backfills. If redistributed, their colleagues absorb extra work. Either way, there's a cost not reflected here.

**[GH]:** Also — if these are truly our best people, we're taking them away from their current high-value work. What's the opportunity cost? Has anyone calculated what they're producing now vs. what they might produce in the Lab?

### What Tools

> The Lab gets access to:
> - Frontier models (Claude, GPT-4, Gemini) via API
> - Coding assistants (Cursor, GitHub Copilot, Claude Code)
> - Experimental tools as they emerge (with Security's input, not sign-off)

**[GH]:** "With Security's input, not sign-off" — this is the line that worries me. Sarah has raised legitimate concerns about data governance. We can't just override her because we want to move fast. What happens when (not if) there's an incident?

### Governance

> - **Weekly check-ins:** Security and Legal have visibility, not veto power

**[GH]:** I understand the frustration with slow approvals, but "visibility, not veto" means we're asking Sarah and Judith to watch things happen that they might otherwise stop. That's an uncomfortable position for them and a liability position for us. Who's accountable when something goes wrong — Jordan or Sarah?

---

## What the Lab Will Produce

> Quarterly deliverables:
>
> 1. **Use case library:** Documented examples of high-value AI applications
> 2. **Failure analysis:** What we tried that didn't work, and why
> 3. **Deployment recommendations:** Which applications are ready for broader rollout
> 4. **Capability assessment:** What the current frontier can and can't do
> 5. **Training materials:** For scaling knowledge to the rest of the organization

**[GH]:** These are outputs, not outcomes. A "use case library" has no value in itself. The question is: did any of these use cases get deployed, and did they produce measurable business results?

Here's what I'd want to see for each quarter:

- Q1: [X] use cases identified, [Y] show preliminary promise, [Z]$ estimated impact if deployed
- Q2: [X] prototypes built, [Y] in pilot with real users, first measurable results from [Z]
- Q3: [X] pilots producing verified results, [Y]$ actual impact measured, [Z] recommended for broader deployment
- Q4: [X] deployed beyond Lab, [Y]$ annualized impact, business case for continuation

Without this kind of specificity, I can't evaluate whether we're on track.

---

## Investment

> | Category | Annual Cost |
> |----------|-------------|
> | Star performer backfill (partial) | $180K |
> | 2 FTE developers | $280K |
> | Tool access (API costs, licenses) | $120K |
> | Infrastructure | $40K |
> | Training and development | $30K |
> | **Total incremental** | **$650K** |
> | Less: existing budget reallocation | ($250K) |
> | **Net new investment** | **$400K** |

**[GH]:** Where does the $250K reallocation come from? Which budget? Which projects are we stopping?

**[GH]:** The $180K for backfill is unrealistic. 8 people at 50% time is 4 FTE equivalents. Even at $100K loaded cost (low), that's $400K in backfill, not $180K. The real incremental cost is closer to $620K net.

> Note: This assumes we reduce EnterpriseAI.ai licenses from 2,000 to 500 (covering employees who actually use it), saving $540K annually.

**[GH]:** This is interesting but it's a separate decision. We can reduce licenses without approving the Lab. Don't bundle them — it looks like you're hiding the true cost of the Lab behind unrelated savings.

---

## Risks and Mitigations

> | Risk | Mitigation |
> |------|------------|
> | Security incident | Contained environment, capable users, clear protocols |
> | No measurable impact | Quarterly milestones; fail-fast approach |

**[GH]:** "Quarterly milestones" — but the milestones aren't defined. "Fail-fast approach" — but there's no definition of what constitutes failure that would trigger stopping. What's the trigger? What's the threshold?

---

## Success Metrics

> After 12 months, we should be able to answer:
>
> 1. How many high-value AI use cases did we discover?
> 2. How many were deployed beyond the Lab?
> 3. What measurable impact did those deployments have?
> 4. How much faster did we move compared to our current process?
> 5. How did Lab participants' capabilities grow?

**[GH]:** These are questions, not metrics. A metric has a number attached. For example:

- "We will identify at least 10 use cases with estimated impact over $100K each"
- "At least 3 will be deployed to production by month 12"
- "Deployed use cases will produce at least $500K in measurable value (cost savings or revenue impact)"

If you can't commit to numbers, that tells me you don't know what success looks like. And if you don't know what success looks like, how will we know if this worked?

---

## Why Now

> The gap between frontier AI capabilities and our deployed tools is widening, not closing. Every quarter we wait:
>
> - Frontier models get more capable
> - Our competitors build more organizational muscle
> - Our best people get more frustrated
> - The eventual catch-up cost increases

**[GH]:** "Our competitors" — which competitors? What are they actually doing? I keep hearing we're behind but I've never seen a competitive analysis that substantiates this. Are Grainger and Fastenal really deploying frontier AI in ways that threaten our market position? Show me.

---

## The Ask

> I'm asking for:
>
> 1. Approval to form the Lab with the investment outlined above
> 2. Authority to select participants with input from their managers
> 3. Governance model as described (Security and Legal visibility, not veto)
> 4. 12-month commitment with quarterly reviews
> 5. Executive sponsor (I'd suggest Carla directly, given the cross-functional nature)

**[GH]:** On #3 — I cannot support removing veto authority from Security and Legal. I'm open to streamlined processes with clear SLAs, but "visibility without veto" creates accountability gaps.

On #4 — 12-month commitment is too long without clearer milestones. I'd support a 6-month initial phase with a go/no-go decision at month 6 based on defined criteria.

On #5 — If Carla sponsors, who's operationally accountable? Not Carla — she can't manage this day-to-day. Is it Jordan? Vikram? Someone else? The org chart for this matters.

---

## Overall Assessment

Jordan — I want to be supportive. I can see you're frustrated, and I don't think the current approach is working either. But this proposal asks me to approve $400K+ (really $600K+) for a 12-month experiment with:

- No defined success metrics (just questions)
- No clear accountability structure
- No kill criteria
- No competitive analysis showing we need to move now
- A governance model that sidelines Security and Legal
- Hidden costs in the investment section

I'm not saying no. I'm saying I can't say yes to *this*.

If you can come back with:
1. Specific, measurable outcomes for each quarter
2. Clear kill criteria (what would cause us to stop)
3. Realistic cost accounting including opportunity cost
4. Evidence that competitors are actually ahead (not just assertions)
5. A governance model Sarah and Judith can live with

...then we can have a different conversation.

— Gene

---

*cc: Vikram Anand (for awareness)*
