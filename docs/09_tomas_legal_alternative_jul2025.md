# MEMO

**From:** Tomás Gallegos, Associate General Counsel  
**To:** Judith Abramowitz, CLO  
**Date:** July 28, 2025  
**Subject:** Alternative Framework for AI Risk Management

---

Judith,

Thank you for the opportunity to share my perspective on our AI governance approach. I want to be clear: I'm not disagreeing with your risk assessment. The concerns you've raised—IP ownership, contractual liability, training data issues—are real. But I believe we've constructed a framework that's optimized for risk avoidance rather than risk management, and that's costing us.

## The Current Approach

Our current process requires sequential sign-off: Security review → Legal review → Procurement review → Pilot approval. Each stage has legitimate concerns. But the cumulative effect is a 4-6 month approval cycle for any new tool or initiative.

In a stable technology environment, this makes sense. In a market where capabilities are changing quarterly, it means we're always evaluating yesterday's tools by the time we approve them.

## What Other Companies Are Doing

I've been tracking how peer companies handle AI governance. What I'm seeing:

**Fast movers with strong risk cultures:**
- Sandbox environments where employees can experiment with frontier tools on non-sensitive data
- Tiered approval: low-risk use cases (drafting, summarization) get blanket approval; high-risk use cases (customer-facing, financial) get individual review
- Legal review in parallel with security, not sequential
- Clear "bright line" rules that empower rather than bottleneck

**Examples of bright-line rules:**
- "You may use AI for any internal document that doesn't contain PII or trade secrets"
- "AI outputs require human review before external distribution"
- "No AI for legal documents, contracts, or financial representations"
- "Experiment freely; deploy carefully"

These companies aren't being reckless. They've decided that some risks are acceptable in exchange for learning velocity.

## The Risk of Our Current Approach

I'd argue we're not actually minimizing risk—we're shifting it. The risks we're taking on:

1. **Shadow AI**: Employees are using personal ChatGPT accounts. We know this. Our current policy forbids it, but we're not enforcing meaningfully. This is worse than sanctioned use because we have zero visibility.

2. **Talent attrition**: Denise's memo documented three departures. High performers want to work with modern tools. Every month we delay, we're selecting for employees who don't care about AI—which may not be who we want.

3. **Competitive lag**: Our competitors are building capabilities we can't match. At some point, this becomes a strategic liability, not just a nice-to-have.

4. **Opportunity cost**: The steering committee has spent hundreds of hours on governance discussions. What if we'd spent that time on experimentation?

## A Proposed Alternative

What if we flipped the model?

**Default: Yes, with guardrails**

1. Blanket approval for low-risk use cases (internal documents, research, drafting)
2. Clear prohibited uses (no PII, no contracts, no customer-facing without review)
3. Required training (2 hours, covering risks and rules)
4. Audit capability (we can see what's being used, even if we're not pre-approving)
5. Rapid review for edge cases (48-hour turnaround, not 6-week turnaround)

**For the Lab specifically:**

The Lab proposal creates a contained environment with selected personnel. This is actually lower risk than broad deployment:
- Smaller attack surface
- More sophisticated users
- Better monitoring
- Clear accountability

We could approve the Lab with specific constraints (data handling protocols, output review requirements, regular compliance check-ins) in weeks, not months.

## The Legal Exposure Question

I know your concern: if something goes wrong, we're liable. But I'd offer two thoughts:

1. Something will go wrong eventually regardless of our approval pace. The question is whether we've built an organization that can recognize, respond, and learn from problems.

2. "We moved cautiously" is not a defense if the caution itself caused harm. If our slow pace leads to competitive failure or talent exodus, that's also a board-level problem.

I'm not asking us to be reckless. I'm asking whether we can be thoughtfully fast rather than thoroughly slow.

Happy to discuss.

Tomás

---

*Note: I've shared this with Jordan Torres, who suggested I send it to you directly. I'm not trying to go around the process—I'm trying to improve it.*
