# MEMO

**From:** Gene Hollister, CFO and Vikram Anand, CTO  
**To:** Carla Benedetti, CEO  
**CC:** Meridian Leadership Team  
**Date:** January 6, 2026  
**Subject:** AI at Meridian - A Year in Review and Path Forward

---

Carla,

As we discussed in December, we've prepared a joint assessment of Meridian's AI initiatives after one full year. We approached this collaboratively because we believe the path forward requires alignment between technology vision and financial discipline.

## What We Set Out to Do

In January 2025, you challenged us to embrace AI as "an organizational transformation, not a technology project." We took that seriously. We deployed EnterpriseAI.ai across the enterprise, stood up a steering committee, launched pilot programs, invested in training, and began building toward more ambitious capabilities.

## What We Spent

Total 2025 AI investment: **$8.0M**

| Category | Amount |
|----------|--------|
| EnterpriseAI.ai licenses (25,000 seats) | $5.4M |
| Consulting and advisory | $800K |
| Training and enablement | $500K |
| Infrastructure and integration | $700K |
| Personnel (AI Initiative team) | $600K |

This excludes indirect costs (time spent in steering committee meetings, security reviews, etc.) which we estimate at $400-500K in opportunity cost.

## What We Got

We want to be honest: the measurable returns are modest.

**EnterpriseAI.ai Adoption**
- 25,000 licenses deployed (all employees)
- 37% monthly active users (~9,200 employees)—actually above industry benchmarks
- But usage concentrated in low-value tasks: email drafting, meeting summaries
- High-value use cases (data analysis, workflow automation): less than 2% of usage
- Advanced capabilities (coding, tool building): less than 0.5% of usage

**Pilot Programs**
- 4 pilots proposed, 1 completed, 1 ongoing, 2 delayed/cancelled
- Customer insights pilot: team reports 20% faster analysis cycle (unverified)
- Marketing pilot: inconclusive results
- Supply chain pilot: delayed to 2026
- R&D pilot: never launched

**Productivity Impact**
- No measurable impact on revenue, cost structure, or cycle times
- Anecdotal reports of individual productivity gains
- No workflow transformations completed

## Our Assessment

We've spent $8M and have little to show for it in terms of measurable business impact. That's a hard sentence to write, but it's honest.

Paradoxically, our adoption numbers are actually above industry benchmarks. We're not failing at getting people to use AI—we're failing at getting AI to change outcomes.

However, we don't agree on why.

**Gene's view:** We moved too fast on adoption and too slow on discipline. We bought 25,000 licenses before we had use cases that justified enterprise-wide deployment. We launched pilots without success criteria. We asked for more investment (the Lab proposal) before demonstrating value from current investment. The right path forward is consolidation: reduce spend, focus on high-value use cases, prove ROI before expanding. At $8M, this isn't a small experiment anymore—it's a major capital allocation decision that has produced no measurable returns.

**Vikram's view:** We moved too slow, but we moved slow for the wrong reasons. Every initiative got bogged down in ad-hoc reviews, informal approvals, and inconsistent process. The problem wasn't that we had governance—it's that we didn't have *enough* governance, applied systematically. When AI decisions happen outside IT's established frameworks, delays are inevitable because we're reinventing the wheel each time. 

The lesson isn't to abandon oversight—it's to professionalize it. We need dedicated AI operations capacity within IT: people whose job is to evaluate, provision, and support AI tools efficiently. We also need formal upskilling so IT staff can conduct AI security and compliance reviews with confidence, rather than defaulting to caution. My team can move fast when we have the resources and training to do so.

## Where We Agree

Despite our different diagnoses, we agree on several things:

1. **The status quo isn't working.** Continuing current trajectory will produce similar results: spend without impact.

2. **We need clearer success metrics.** "AI transformation" is too vague. We need specific, measurable objectives.

3. **The Lab proposal, as written, isn't ready.** Jordan's proposal has merit, but it doesn't answer basic questions: What will the Lab produce? How will we measure success? What's the exit criteria?

4. **We're not a technology company.** Meridian's competitive advantage is operational excellence, customer relationships, and domain expertise—not technical innovation. We should be fast followers, not pioneers. The risk of moving too slow is real; the risk of moving too fast into unproven territory is also real.

This is worth dwelling on. There's a temptation—visible in some internal proposals—to believe that our AI challenges would be solved if only we had access to the "latest" models, the "frontier" tools, the cutting-edge capabilities that seem to change every few weeks. This framing positions Meridian as perpetually behind, always chasing the next release, never reaching a stable operational state.

*Vikram's perspective:* I follow model releases closely. Yes, each new version does marginally better on benchmarks measuring advanced mathematics or complex coding tasks—but that's hardly our use case. And these releases are not without risk. When OpenAI rushed out GPT-5.1 in June, it had significant behavioral regressions and they had to push a patched version within weeks. Anthropic has had similar growing pains with rapid Claude 4 releases. I would much rather have our employees using a proven model like GPT-5.0 or Claude 4 Sonnet for an extra few months than jump to GPT-5.2 or Claude 4.5 before we have any idea whether it's actually better for summarizing meeting notes and drafting customer emails—which is what our people actually do with these tools.

We should be skeptical of proposals that treat "frontier model access" as the solution to organizational challenges. The solution is disciplined adoption of proven, stable tools through established IT channels—with proper evaluation before deployment.

## Recommendation

We propose a **90-day strategy reset** with parallel capacity-building:

**Immediate pause:**
1. **Pause** new AI investments pending clearer strategic framework
2. **Audit** current tools and pilots for actual usage and value
3. **Define** 3-5 specific, measurable use cases for 2026
4. **Evaluate** the Lab proposal against revised success criteria
5. **Decide** on 2026 investment level by end of Q1

**Parallel investment (Vikram's recommendation, Gene's partial support):**

To accelerate future AI decisions while maintaining appropriate controls, we should invest in IT capacity:

- **Add 3 FTEs to IT** with AI operations responsibilities: one focused on security/compliance review, one on integration/provisioning, one on user support and training. This addresses the bottleneck that delayed our pilots. Estimated cost: $420K annually.

- **Engage Global Consulting Partners for AI Compliance Certification program** for existing IT staff (12 participants). This is their "Enterprise AI Governance" curriculum, a 6-month program with certification. Graduates will be equipped to conduct AI security reviews confidently and efficiently. Estimated cost: $180K.

Gene's note: *I support investing in IT capacity if it genuinely accelerates decisions. I'm less certain we need an external certification program—this feels like it could become another delay mechanism. But I defer to Vikram's judgment on what his team needs.*

Vikram's note: *The certification program is important. Our security review delays aren't because my team is obstructionist—they're because we're being asked to evaluate tools we weren't trained to evaluate. Professional development solves this.*

**Total incremental investment:** $600K (partially offset if we proceed with license consolidation—reducing from 25,000 to 10,000-15,000 knowledge worker licenses could save $2-3M annually)

We recognize this may be perceived as "slowing down" at a moment when others are speeding up. But disciplined investment beats undisciplined experimentation. Meridian didn't become an industry leader by chasing every trend. We became a leader by making smart bets at the right time.

We're ready to discuss at your convenience.

Gene Hollister | Vikram Anand
